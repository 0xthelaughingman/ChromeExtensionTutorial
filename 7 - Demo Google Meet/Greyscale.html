<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Display Webcam Stream</title>
<script>

//  Globals for state variables
var imageCapture;
var original_stream;
var net ;
var first_add = true;

var scripts = [
        'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.2',
        'https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0',
    ];

function loadScripts(scripts){
    var script = scripts.shift();
    var el = document.createElement('script');
    el.onload = function(script){
        console.log(script , ' loaded!');
        if (scripts.length) {
            loadScripts(scripts);
        }
        else {
            console.log('loaded all scripts');
            loadPix();
        }
    };
    el.src = script;
    document.head.appendChild(el);
}

loadScripts(scripts);

function add_dynamic_elements(constraints)
{
    /*  
        Primary canvas that serves as the source of the modified stream which is passed onto the getUserMedia API.
        Resolution needs to be what the User's device supports.
        Check constraints.txt for sample JSON.
    */

    var invisible_canvas = document.createElement('CANVAS');
    invisible_canvas.id = "invisible";
    invisible_canvas.height = constraints.video.advanced[1].height.min;
    invisible_canvas.width = constraints.video.advanced[2].width.min;
    invisible_canvas.style.position= "absolute";
    invisible_canvas.style.right = -constraints.video.advanced[2].width.min;

    /*  
        A secondary canvas to render the image that goes into the TFJS module. We can't use the first canvas as that is the 
        one that generates the stream.
        Resolution needs to be what the User's device supports.
    */

    //  Div that keeps the content out of the viewport, disables overflow so no scrollbars!
    var wrap_div = document.createElement("div");
    wrap_div.id = "wrap_div";
    wrap_div.style.position= "relative";
    wrap_div.style.right = "0";
    wrap_div.style.bottom = "0px";
    wrap_div.style.width= "0";
    wrap_div.style.height= "0";
    wrap_div.style.overflow= "hidden";


    //  Append all dynamics to the Div
    wrap_div.appendChild(invisible_canvas);
    

    //  Finally append the Div
    document.body.appendChild(wrap_div);
    first_add = false

    
}

var test_div = document.createElement("div");
test_div.innerHTML="TEST";

function override_getUserMedia()
{
    let originalMediaDevicesGetUserMedia = navigator.mediaDevices.getUserMedia;

    navigator.mediaDevices.getUserMedia = function getUserMedia(constraints) { 
    return new Promise((resolve, reject) => {
        console.log("Taking over the implementation!")
        console.log("Original Constraints:\n" , JSON.stringify(constraints))
        originalMediaDevicesGetUserMedia.bind(navigator.mediaDevices)(constraints)
        .then(stream => resolve(get_canvas_stream(stream, constraints))) //this is where we'd divert the stream to a call that modifies it, before resolving the promise
        .catch(reject);
    });
    }
}
function get_canvas_stream(stream, constraints)
{   
    //  set original_stream
    original_stream = stream;
    // log original Video stream's constraints
    
    document.body.appendChild(test_div);
    var stream_new;
    
    if(constraints.video)
    {   
        console.log("ORIG VIDEO META:" , JSON.stringify(stream.getVideoTracks()[0].getCapabilities()))
        //  Change the video stream and if needed, add Audio to the new stream
        
        if(first_add)
        {
            add_dynamic_elements(constraints)
        }
        var canvas = document.getElementById("invisible");
        stream_new = canvas.captureStream(24);

        //  log new stream's constraints
        console.log("NEW VIDEO META:" , JSON.stringify(stream_new.getVideoTracks()[0].getCapabilities()))
        if(constraints.audio)
        {
            stream_new.addTrack(stream.getAudioTracks()[0]);
            console.log("NEW AUDIO META:" , JSON.stringify(stream_new.getAudioTracks()[0].getCapabilities()))
        }
        audioTimerLoop(nextFrame, 60)
    }
    else if (constraints.audio)
    {   
        //  only audio, just let the original stream be as is.
        console.log("Audio Only")
        stream_new = stream
        console.log("NEW AUDIO META:" , JSON.stringify(stream_new.getAudioTracks()[0].getCapabilities()))
    }

    //  feed the stream from the canvas
    return stream_new;
}

//  FrameLooper
function nextFrame() {
    var track = original_stream.getVideoTracks()[0];
    imageCapture = new ImageCapture(track);

    if ((imageCapture.track.readyState == 'live' ))
    {
        imageCapture.grabFrame()
        .then(imageBitmap => {
            //  const canvas = document.querySelector('#canvas');
            var canvas = document.getElementById("invisible");
            drawCanvas(canvas, imageBitmap, "grayscale");
        }).catch(error => console.log(error));
    }
    else
    {
        console.log(imageCapture.track.readyState)
    }
    //requestAnimationFrame(nextFrame)
    //console.log("looping")
    
}

async function drawCanvas(canvas, img, draw_type) {

    if(draw_type ==="grayscale")
    {
        draw_grayscale(canvas,img)
    }

}

function draw_image(canvas, img)
{
    canvas.width = getComputedStyle(canvas).width.split('px')[0];
    canvas.height = getComputedStyle(canvas).height.split('px')[0];
    let ratio  = Math.min(canvas.width / img.width, canvas.height / img.height);
    let x = (canvas.width - img.width * ratio) / 2;
    let y = (canvas.height - img.height * ratio) / 2;
    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
    canvas.getContext('2d').drawImage(img, 0, 0, img.width, img.height,
        x, y, img.width * ratio, img.height * ratio);

}

function draw_grayscale(canvas,img)
{
    canvas.width = getComputedStyle(canvas).width.split('px')[0];
    canvas.height = getComputedStyle(canvas).height.split('px')[0];
    let ratio  = Math.min(canvas.width / img.width, canvas.height / img.height);
    let x = (canvas.width - img.width * ratio) / 2;
    let y = (canvas.height - img.height * ratio) / 2;
    canvas.getContext('2d').filter="grayscale(50)";
    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
    canvas.getContext('2d').drawImage(img, 0, 0, img.width, img.height,
        x, y, img.width * ratio, img.height * ratio);
}

override_getUserMedia()

function get_cam()
{   var audio_constraints = {"audio":{"mandatory":{"sourceId":"default"},"optional":[{"googEchoCancellation":true},{"googEchoCancellation2":true},{"googAutoGainControl":true},{"googNoiseSuppression":true},{"googHighpassFilter":true},{"googAudioMirroring":true}]},"video":false}
    var vid_constraints = {"audio":false,"video":{"advanced":[{"frameRate":{"min":24}},{"height":{"min":720}},{"width":{"min":1280}},{"frameRate":{"max":24}},{"width":{"max":1280}},{"height":{"max":720}},{"aspectRatio":{"exact":1.7777777777777777}}]}}
    var video_cont = document.querySelector("#videoElement");
    if (navigator.mediaDevices.getUserMedia) {
    navigator.mediaDevices.getUserMedia(vid_constraints)
        .then(function (stream) {
        video_cont.srcObject = stream;
        })
        .catch(function (err0r) {
        console.log("Something went wrong!" , err0r.message);
        });
    }
}

async function loadPix(){
    //   net = await bodyPix.load();
    net = await bodyPix.load({
    architecture: 'MobileNetV1',
    outputStride: 16,
    multiplier: 0.75,
    quantBytes: 2
    });

    console.log("Loaded BodyPix!")
}


/*
    An alternative timing loop, based on AudioContext's clock

    @arg callback : a callback function 
        with the audioContext's currentTime passed as unique argument
    @arg frequency : float in ms;
    

*/
function audioTimerLoop(callback, frequency) {

    var freq = frequency / 1000;      // AudioContext time parameters are in seconds
    var aCtx = new AudioContext();
    // Chrome needs our oscillator node to be attached to the destination
    // So we create a silent Gain Node
    var silence = aCtx.createGain();
    silence.gain.value = 0;
    silence.connect(aCtx.destination);

    onOSCend();

         
    function onOSCend() 
    {
        var osc = aCtx.createOscillator();
        osc.onended = onOSCend; // so we can loop
        osc.connect(silence);
        osc.start(0); // start it now
        osc.stop(aCtx.currentTime + freq); // stop it next frame
        callback(aCtx.currentTime); // one frame is done
        
    };
    
}
</script>


<style>
#container {
    margin: 0px;
    width: 1280px;
    height: 720px;
    border: 10px #333 solid;
}
#videoElement {
    width: 1280px;
    height: 720px;
    background-color: #666;
}

</style>
</head>
 
<body id="body">
<div id="container">
	<video autoplay="true" id="videoElement">
	
	</video>
</div>

<div>
    <button onclick="get_cam()" >Click to stream video!</button>
</div>
</body>
<footer>test</footer>
</html>