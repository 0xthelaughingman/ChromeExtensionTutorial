<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Display Webcam Stream</title>
<script>

//  Globals for state variables
var imageCapture;
var original_stream;
var net ;

//  Globals for dynamic HTML elements to be added.
var invisible_canvas;
var img_canvas;
var wrap_div;

var scripts = [
        'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.2',
        'https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0',
    ];

function loadScripts(scripts){
    var script = scripts.shift();
    var el = document.createElement('script');
    el.onload = function(script){
        console.log(script + ' loaded!');
        if (scripts.length) {
            loadScripts(scripts);
        }
        else {
            console.log('loaded all scripts');
            loadPix();
        }
    };
    el.src = script;
    document.head.appendChild(el);
}

loadScripts(scripts);

function add_dynamic_elements()
{
    /*  
        Primary canvas that serves as the source of the modified stream which is passed onto the getUserMedia API.
        Resolution needs to be what the User's device supports.
    */
    invisible_canvas = document.createElement('CANVAS');
    invisible_canvas.id = "invisible";
    invisible_canvas.height = 375;
    invisible_canvas.width = 500;
    invisible_canvas.style.position= "absolute";
    invisible_canvas.style.top = "-500";

    /*  
        A secondary canvas to render the image that goes into the TFJS module. We can't use the first canvas as that is the 
        one that generates the stream.
        Resolution needs to be what the User's device supports.
    */
    img_canvas = document.createElement('CANVAS');
    img_canvas.id = "img_canvas";
    img_canvas.height = 375;
    img_canvas.width = 500;


    //  Div that keeps the content out of the viewport, disables overflow so no scrollbars!
    wrap_div = document.createElement("div");
    wrap_div.id = "wrap_div";
    wrap_div.style.position= "relative";
    wrap_div.style.right = "0";
    wrap_div.style.bottom = "0px";
    wrap_div.style.width= "0";
    wrap_div.style.overflow= "hidden";


    //  Append all dynamics to the Div
    wrap_div.appendChild(invisible_canvas);
    wrap_div.appendChild(img_canvas);

    //  Finally append the Div
    document.body.appendChild(wrap_div);
    
}





var test_div = document.createElement("div");
test_div.innerHTML="TEST";

function override_getUserMedia()
{
    let originalMediaDevicesGetUserMedia = navigator.mediaDevices.getUserMedia;

    navigator.mediaDevices.getUserMedia = function getUserMedia(constraints) { 
    return new Promise((resolve, reject) => {
        console.log("Taking over the implementation!")
        originalMediaDevicesGetUserMedia.bind(navigator.mediaDevices)(constraints)
        .then(stream => resolve(get_canvas_stream(stream))) //this is where we'd divert the stream to a call that modifies it, before resolving the promise
        .catch(reject);
    });
    }
}
function get_canvas_stream(stream)
{   
    //set original_stream
    original_stream = stream;
    add_dynamic_elements()
    document.body.appendChild(test_div);

    var canvas = document.getElementById("invisible");
    const stream_new = canvas.captureStream(30);
    stream_new.addTrack(original_stream.getAudioTracks()[0])
    nextFrame();

    // feed the stream from the canvas
    return stream_new;
}

//FrameLooper
function nextFrame() {
    var track = original_stream.getVideoTracks()[0];
    imageCapture = new ImageCapture(track);

    if ((imageCapture.track.readyState == 'live' ))
    {
        imageCapture.grabFrame()
        .then(imageBitmap => {
            //const canvas = document.querySelector('#canvas');
            var canvas = document.getElementById("invisible");
            drawCanvas(canvas, imageBitmap);
        }).catch(error => console.log(error));
    }
    else
    {
        console.log(imageCapture.track.readyState)
    }
    requestAnimationFrame(nextFrame)
}
function draw_image(canvas, img)
{
    canvas.width = getComputedStyle(canvas).width.split('px')[0];
    canvas.height = getComputedStyle(canvas).height.split('px')[0];
    let ratio  = Math.min(canvas.width / img.width, canvas.height / img.height);
    let x = (canvas.width - img.width * ratio) / 2;
    let y = (canvas.height - img.height * ratio) / 2;
    //canvas.getContext('2d').filter="grayscale(50)";
    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
    canvas.getContext('2d').drawImage(img, 0, 0, img.width, img.height,
        x, y, img.width * ratio, img.height * ratio);

}


/*
    This time around we are going to :
    1. Render the stream's image onto a secondary canvas
    2. Use that canvas' feed into the TFJS model
    3. Draw the model's result onto our primary canvas

*/
async function drawCanvas(canvas, img) {

    draw_image(img_canvas, img);
    try
    {
        const partSegmentation = await net.segmentMultiPersonParts(img_canvas);

        // The colored part image is an rgb image with a corresponding color from the
        // rainbow colors for each part at each pixel, and black pixels where there is
        // no part.
        const coloredPartImage = bodyPix.toColoredPartMask(partSegmentation);
        const opacity = 1;
        const flipHorizontal = false;
        const maskBlurAmount = 0;
        // Draw the colored part image on top of the original image onto a canvas.
        // The colored part image will be drawn semi-transparent, with an opacity of
        // 0.7, allowing for the original image to be visible under.
        bodyPix.drawPixelatedMask(
            canvas, img_canvas, coloredPartImage, opacity, maskBlurAmount,
            flipHorizontal);
    }
    catch(error)
    {
        console.log(error)
        console.log("Filling black rect as backup.")
    }
}

override_getUserMedia()

function get_cam()
{
    var video_cont = document.querySelector("#videoElement");
    if (navigator.mediaDevices.getUserMedia) {
    navigator.mediaDevices.getUserMedia({ video: true, audio:true })
        .then(function (stream) {
        video_cont.srcObject = stream;
        })
        .catch(function (err0r) {
        console.log("Something went wrong!" + err0r);
        });
    }
}

async function loadPix(){
   //net = await bodyPix.load();
    net = await bodyPix.load({
    architecture: 'MobileNetV1',
    outputStride: 16,
    multiplier: 0.75,
    quantBytes: 2
    });

    console.log("Loaded BodyPix!")
}
</script>


<style>
#container {
    margin: 0px auto;
    width: 500px;
    height: 375px;
    border: 10px #333 solid;
}
#videoElement {
    width: 500px;
    height: 375px;
    background-color: #666;
}
#canvas
{
    display: none;
    width: 500px;
    height: 375px;
    background-color: #666;

}

</style>
</head>
 
<body id="body">
<div id="container">
	<video autoplay="true" id="videoElement">
	
	</video>
</div>

<div>
    <button onclick="get_cam()" >Click to stream video!</button>
</div>
</body>
<footer>test</footer>
</html>