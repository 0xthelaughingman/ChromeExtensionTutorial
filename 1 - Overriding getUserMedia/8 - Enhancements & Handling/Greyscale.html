<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Display Webcam Stream</title>
<script>

//  Globals for state variables
var imageCapture;
var original_stream;
var canvas_stream;
var net ;
var video_on = false;

//  dimension globals for secondary canvas
var draw_canvas_height;
var draw_canvas_width;

var scripts = [
        'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.2',
        'https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0',
    ];

function loadScripts(scripts){
    var script = scripts.shift()
    var el = document.createElement('script')
    el.onload = function(script){
        console.log(script , ' loaded!');
        if (scripts.length) {
            loadScripts(scripts)
        }
        else {
            console.log('loaded all scripts')
            loadPix()
        }
    };
    el.src = script
    document.head.appendChild(el)
}

loadScripts(scripts);

function add_dynamic_elements(constraints)
{
    /*  
        Primary canvas that serves as the source of the modified stream which is passed onto the getUserMedia API.
        Resolution needs to be what the User's device supports.
        Check constraints.txt for sample JSON.
    */

    var invisible_canvas = document.createElement('CANVAS')
    invisible_canvas.id = "invisible"
    invisible_canvas.height = constraints.video.advanced[1].height.min
    invisible_canvas.width = constraints.video.advanced[2].width.min
    invisible_canvas.style.position= "absolute"
    invisible_canvas.style.right = -constraints.video.advanced[2].width.min

    draw_canvas_height = invisible_canvas.height
    draw_canvas_width = invisible_canvas.width

    /*  
        A secondary canvas needed to render the image that goes into the TFJS module. We can't use the first canvas as that is the 
        one that generates the stream.
        Resolution needs to be what the User's device supports.
    */
    //  Add code for the seconday canvas here



    //  Div that keeps the content out of the viewport, disables overflow so no scrollbars!
    var wrap_div = document.createElement("div")
    wrap_div.id = "wrap_div"
    wrap_div.style.position= "relative"
    wrap_div.style.right = "0"
    wrap_div.style.bottom = "0px"
    wrap_div.style.width= "0"
    wrap_div.style.height= "0"
    wrap_div.style.overflow= "hidden"

    //  Append all dynamics to the Div
    wrap_div.appendChild(invisible_canvas);

    //  Finally append the Div
    document.body.appendChild(wrap_div);

}

function remove_dynamic_elements()
{
    var dyn = document.getElementById("wrap_div")
    if(dyn)
    {
        document.body.removeChild(dyn)
    }
}

function override_getUserMedia()
{
    let originalMediaDevicesGetUserMedia = navigator.mediaDevices.getUserMedia;

    navigator.mediaDevices.getUserMedia = function getUserMedia(constraints) { 
    return new Promise((resolve, reject) => {
        console.log("Original Constraints:\n" , JSON.stringify(constraints))
        originalMediaDevicesGetUserMedia.bind(navigator.mediaDevices)(constraints)
        .then(stream => resolve(get_canvas_stream(stream, constraints)))    //  this is where we'd divert the stream to a call that modifies it, before resolving the promise
        .catch(reject)
    });
    }
}

function get_canvas_stream(stream, constraints)
{   
    var stream_new;

    if(constraints.video)
    {   
        //  Check if it's a screen share!
        if( JSON.stringify(constraints).indexOf("chromeMediaSource") > -1 ) {
            console.log("Returning Screenshare");
            return stream
        }
        
        original_stream = stream
        console.log("ORIG VIDEO META:", 
                        JSON.stringify(stream.getVideoTracks()[0].getCapabilities()))
        video_on = false
        //  Adapt the canvas to new contraints, remove previous artifacts
        remove_dynamic_elements()
        add_dynamic_elements(constraints)

        var canvas = document.getElementById("invisible")
        stream_new = canvas.captureStream(24)
        canvas_stream = stream_new  //  Maintain global ref. to the stream
        //  log new stream's constraints
        console.log("NEW VIDEO META:", 
                        JSON.stringify(stream_new.getVideoTracks()[0].getCapabilities()))

        if(constraints.audio)
        {
            stream_new.addTrack(stream.getAudioTracks()[0]);
            console.log("AUDIO META:", 
                            JSON.stringify(stream_new.getAudioTracks()[0].getCapabilities()))
        }

        video_on = true; // audioTimer's loop condition.
        audioTimerLoop(nextFrame, 60)
        return stream_new
    }
    else if (constraints.audio)
    {   
        //  only audio, just let the original stream be as is.
        console.log("Audio Only")
        console.log("AUDIO META:", 
                        JSON.stringify(stream.getAudioTracks()[0].getCapabilities()))
        return stream
    }

    console.log("Default return")
    return stream;
}


//  FrameLooper
function nextFrame() {
    /*
        Since we have 2 streams running, and only the canvas stream goes to the app... Only the canvas stream
        will have its' tracks' stop() called when we stop video on the app.
        We have to call the original stream's stop on the video track manually! 
        Otherwise the Camera will still remain active!
    */

    var canvas_track = canvas_stream.getVideoTracks()[0]
    if(canvas_track.readyState != 'live')
    {
        console.log("Video Stopped by App")
        original_stream.getVideoTracks()[0].stop()
        video_on = false
        remove_dynamic_elements()
        return
    }

    var track = original_stream.getVideoTracks()[0];
    imageCapture = new ImageCapture(track)

    if ((imageCapture.track.readyState == 'live' ))
    {
        imageCapture.grabFrame()
        .then(imageBitmap => {
            var canvas = document.getElementById("invisible")
            drawCanvas(canvas, imageBitmap, "grayscale")
        })
        .catch(error => 
                console.log(imageCapture.track.readyState, error)
            );
    }
    else
    {
        console.log(imageCapture.track.readyState)
    }
}

async function drawCanvas(canvas, img, draw_type) {

    if(draw_type ==="grayscale")
    {
        draw_grayscale(canvas,img)
    }
}

function draw_image(canvas, img)
{
    canvas.getContext('2d').drawImage(img, 0, 0)
}

function draw_grayscale(canvas,img)
{
    canvas.getContext('2d').filter="grayscale(50)"
    //  canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
    canvas.getContext('2d').drawImage(img, 0, 0)
}

override_getUserMedia()

async function loadPix(){
    //   net = await bodyPix.load();
    net = await bodyPix.load({
    architecture: 'MobileNetV1',
    outputStride: 16,
    multiplier: 0.75,
    quantBytes: 2
    });
    console.log("Loaded BodyPix!")
}

/*
    An alternative timing loop, based on AudioContext's clock
    @arg callback : a callback function with the audioContext's currentTime passed as unique argument
    @arg frequency : float in ms;
*/
function audioTimerLoop(callback, frequency) {

    var freq = frequency/1000   //  AudioContext time parameters are in seconds
    var aCtx = new AudioContext()
    //  Chrome needs our oscillator node to be attached to the destination
    //  So we create a silent Gain Node
    var silence = aCtx.createGain()
    silence.gain.value = 0
    silence.connect(aCtx.destination)

    onOSCend();

    function onOSCend() 
    {
        var osc = aCtx.createOscillator()
        osc.onended = onOSCend  //  so we can loop
        osc.connect(silence)
        osc.start(0)    //  start it now
        osc.stop(aCtx.currentTime + freq)   //  stop it next frame
        callback(aCtx.currentTime)  //  one frame is done

        if (!video_on) {    //  user broke the loop
            osc.onended = function() {
            aCtx.close();   //  clear the audioContext
            console.log("Exiting Timer loop", video_on)
            return
            };
        }
    };
    
}

function get_cam()
{   
    var audio_constraints = {"audio":{"mandatory":{"sourceId":"default"},"optional":[{"googEchoCancellation":true},{"googEchoCancellation2":true},{"googAutoGainControl":true},{"googNoiseSuppression":true},{"googHighpassFilter":true},{"googAudioMirroring":true}]},"video":false}
    var vid_constraints = {"audio":true,"video":{"advanced":[{"frameRate":{"min":24}},{"height":{"min":720}},{"width":{"min":1280}},{"frameRate":{"max":24}},{"width":{"max":1280}},{"height":{"max":720}},{"aspectRatio":{"exact":1.7777777777777777}}]}}
    var video_cont = document.querySelector("#videoElement");
    if (navigator.mediaDevices.getUserMedia) {
    navigator.mediaDevices.getUserMedia(vid_constraints)
        .then(function (stream) {
        video_cont.srcObject = stream;
        })
        .catch(function (err0r) {
        console.log("Something went wrong!", err0r.message)
        });
    }
}
</script>


<style>
#container {
    margin: 0px;
    width: 1280px;
    height: 720px;
    border: 10px #333 solid;
}
#videoElement {
    width: 1280px;
    height: 720px;
    background-color: #666;
}

</style>
</head>
 
<body id="body">
<div id="container">
	<video autoplay="true" id="videoElement">
	
	</video>
</div>

<div>
    <button onclick="get_cam()" >Click to stream video!</button>
</div>
</body>
<footer>test</footer>
</html>