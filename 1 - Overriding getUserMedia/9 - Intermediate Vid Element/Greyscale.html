<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Display Webcam Stream</title>
<script>

//  Globals for state variables
var imageCapture
var original_stream
var canvas_stream
var net 
var video_on = false
var draw_type = "grayscale" //  This should have a function defined that's called in get_canvas_stream that reads from the Extension's persistent data,
                            //  which the user updates/selects.
                            //  Current supported values : "grayscale", "tfjs-pixel"

//  dimension constraint globals for TFJS rendering
var tfjs_720p = {"height":{"max":720,"min":1}, "width":{"max":1280,"min":1}}
var tfjs_360p = {"height":{"max":360,"min":1}, "width":{"max":480,"min":1}}
var tfjs_240p = {"height":{"max":240,"min":1}, "width":{"max":352,"min":1}}


var scripts = [
        'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.2',
        'https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0',
    ];

function loadScripts(scripts){
    var script = scripts.shift()
    var el = document.createElement('script')
    el.onload = function(script){
        console.log(script , ' loaded!');
        if (scripts.length) {
            loadScripts(scripts)
        }
        else {
            console.log('loaded all scripts')
            loadPix()
        }
    };
    el.src = script
    document.head.appendChild(el)
}

loadScripts(scripts);

function add_dynamic_elements(constraints)
{
    /*  
        Primary canvas that serves as the source of the modified stream which is passed onto the getUserMedia API.
        Resolution needs to be what the User's device supports.
        Check constraints.txt for sample JSON.
    */

    var invisible_canvas = document.createElement('CANVAS')
    invisible_canvas.id = "invisible"
    invisible_canvas.height = constraints.height.max
    invisible_canvas.width = constraints.width.max
    invisible_canvas.style.position= "absolute"
    invisible_canvas.style.right = -constraints.width.max

    /*  
        A secondary canvas/video needed to render the image that goes into the TFJS module. We can't use the first canvas as that is the 
        one that generates the stream.
        Resolution needs to be what the User's device supports.
    */
    var invisible_video = document.createElement('video')
    invisible_video.id = "invisible_video"
    invisible_video.height = constraints.height.max
    invisible_video.width = constraints.width.max
    invisible_video.style.position= "absolute"
    invisible_video.style.right = -(2 * constraints.width.max)

    //  Div that keeps the content out of the viewport, disables overflow so no scrollbars!
    var wrap_div = document.createElement("div")
    wrap_div.id = "wrap_div"
    wrap_div.style.position= "relative"
    wrap_div.style.right = "0"
    wrap_div.style.bottom = "0px"
    wrap_div.style.width= "0"
    wrap_div.style.height= "0"
    //wrap_div.style.overflow= "hidden"

    //  Append all dynamics to the Div
    wrap_div.appendChild(invisible_canvas);
    wrap_div.appendChild(invisible_video);

    //  Finally append the Div
    document.body.appendChild(wrap_div);
    console.log("Added Div")
}

function remove_dynamic_elements()
{
    var dyn = document.getElementById("wrap_div")
    if(dyn)
    {
        document.body.removeChild(dyn)
    }
}

function override_getUserMedia()
{
    let originalMediaDevicesGetUserMedia = navigator.mediaDevices.getUserMedia;

    navigator.mediaDevices.getUserMedia = function getUserMedia(constraints) { 
    return new Promise((resolve, reject) => {
        console.log("Original Requested Constraints:\n" , JSON.stringify(constraints))
        originalMediaDevicesGetUserMedia.bind(navigator.mediaDevices)(constraints)
        .then(stream => resolve(get_canvas_stream_beta(stream, constraints)))    //  this is where we'd divert the stream to a call that modifies it, before resolving the promise
        .catch(reject)
    });
    }
}

function get_canvas_stream(stream, constraints)
{   
    var stream_new;

    if(constraints.video)
    {   
        //  Check if it's a screen share!
        if( JSON.stringify(constraints).indexOf("chromeMediaSource") > -1 ) {
            console.log("Returning Screenshare");
            return stream
        }
        //  Previous states/track cleanup if any.
        video_on = false
        if (canvas_stream!=null)
            canvas_stream.getVideoTracks()[0].stop()
        if (original_stream!=null)
            original_stream.getVideoTracks()[0].stop()
        

        original_stream = stream
        var original_constraints = stream.getVideoTracks()[0].getCapabilities()
        console.log("ORIG VIDEO META:", JSON.stringify(original_constraints))


        //  Adapt the canvas to new contraints, remove previous artifacts
        remove_dynamic_elements()
        add_dynamic_elements(constraints)

        var canvas = document.getElementById("invisible")
        stream_new = canvas.captureStream(24)
        canvas_stream = stream_new  //  Maintain global ref. to the stream


        var track = original_stream.getVideoTracks()[0];    //  Setup imageCapture for Animations onto the canvas
        imageCapture = new ImageCapture(track)
        //  log new stream's constraints
        console.log("NEW VIDEO META:", 
                        JSON.stringify(stream_new.getVideoTracks()[0].getCapabilities()))

        if(constraints.audio)
        {
            stream_new.addTrack(stream.getAudioTracks()[0]);
            console.log("AUDIO META:", 
                            JSON.stringify(stream_new.getAudioTracks()[0].getCapabilities()))
        }

        video_on = true; // audioTimer's loop condition.
        audioTimerLoop(nextFrame, 60)
        return stream_new
    }
    else if (constraints.audio)
    {   
        //  only audio, just let the original stream be as is.
        console.log("Audio Only")
        console.log("AUDIO META:", 
                        JSON.stringify(stream.getAudioTracks()[0].getCapabilities()))
        return stream
    }

    console.log("Default return")
    return stream;
}

function get_canvas_stream_beta(stream, constraints)
{   
    var stream_new;

    if(constraints.video)
    {   
        //  Check if it's a screen share!
        if( JSON.stringify(constraints).indexOf("chromeMediaSource") > -1 ) {
            console.log("Returning Screenshare");
            return stream
        }

        //  Previous states/track cleanup if any.
        video_on = false
        if (canvas_stream!=null)
            canvas_stream.getVideoTracks()[0].stop()
        if (original_stream!=null)
            original_stream.getVideoTracks()[0].stop()
        

        original_stream = stream
        var original_constraints = stream.getVideoTracks()[0].getCapabilities()
        console.log("ORIG VIDEO META:", JSON.stringify(original_constraints))


        //  Adapt the canvas to new contraints, remove previous artifacts
        remove_dynamic_elements()

        var new_constraints;
        if (draw_type==="tfjs")
            new_constraints = tfjs_240p
        else
            new_constraints = original_constraints

        add_dynamic_elements(new_constraints)

        var video = document.getElementById("invisible_video")

        var canvas = document.getElementById("invisible")
        stream_new = canvas.captureStream(24)
        canvas_stream = stream_new  //  Maintain global ref. to the stream

        video.srcObject = stream
        video.play()

        //  log new stream's constraints
        console.log("NEW VIDEO META:", 
                        JSON.stringify(stream_new.getVideoTracks()[0].getCapabilities()))

        if(constraints.audio)
        {
            stream_new.addTrack(stream.getAudioTracks()[0]);
            console.log("AUDIO META:", 
                            JSON.stringify(stream_new.getAudioTracks()[0].getCapabilities()))
        }

        video_on = true; // audioTimer's loop condition.
        audioTimerLoop(nextVideoFrame, 60)
        //nextVideoFrame()
        return stream_new
    }
    else if (constraints.audio)
    {   
        //  only audio, just let the original stream be as is.
        console.log("Audio Only")
        console.log("AUDIO META:", 
                        JSON.stringify(stream.getAudioTracks()[0].getCapabilities()))
        return stream
    }

    console.log("Default return")
    return stream;
}



/*
    Root function for drawing each frame, passed as callback to the AudioTimer.
    Not using requestAnimationFrame() callback as that gets suspended on tab switch/minimize, can't have that on a live video stream.
*/
function nextFrame() {
    /*
        Since we have 2 streams running, and only the canvas stream goes to the app... Only the canvas stream
        will have its' tracks' stop() called when we stop video on the app.
        We have to call the original stream's stop on the video track manually! 
        Otherwise the Camera will still remain active!
    */

    var canvas_track = canvas_stream.getVideoTracks()[0]
    if(canvas_track.readyState != 'live')
    {
        console.log("Video Stopped")
        original_stream.getVideoTracks()[0].stop()
        video_on = false
        imageCapture = null
        remove_dynamic_elements()
        return
    }
    
    if (imageCapture.track.readyState == 'live' && imageCapture.track.enabled && !imageCapture.track.muted) 
    {
        imageCapture.grabFrame()
        .then(imageBitmap => {
            var canvas = document.getElementById("invisible")
            //console.log("Canvas was found!")
            drawCanvas(canvas, imageBitmap, "grayscale")
        })
        .catch(error => 
                {
                    console.log("Failed canvas animation due to:\n", imageCapture.track.readyState, imageCapture.track.enabled , !imageCapture.track.muted, error)
                    //  Something breaks randomly even with track state==live, haven't figured out why.
                }
            );
    }
    else
    {
        console.log("Not animating due to:", imageCapture.track.readyState, imageCapture.track.enabled , !imageCapture.track.muted)
        //  Track gets muted for some reason... No recovery till stopped and restarted...
        canvas_stream.getVideoTracks()[0].stop()
    }
}

function nextVideoFrame() {
    /*
        Since we have 2 streams running, and only the canvas stream goes to the app... Only the canvas stream
        will have its' tracks' stop() called when we stop video on the app.
        We have to call the original stream's stop on the video track manually! 
        Otherwise the Camera will still remain active!
    */
    var video = document.getElementById("invisible_video")
    var canvas_track = canvas_stream.getVideoTracks()[0]

    if(canvas_track.readyState != 'live')
    {
        console.log("Video Stopped")
        original_stream.getVideoTracks()[0].stop()
        video_on = false
        //video.stop()
        remove_dynamic_elements()
        return
    }

    var canvas = document.getElementById("invisible")
    drawCanvas(canvas, video, draw_type)
}

/*
    Base function for standard 2D filters.
    The drawtype should ideally be fetched from extension config, controlled/edited by the user.
    Should be switchable on the fly!
*/
function drawCanvas(canvas, img, draw_type) {

    switch(draw_type)
    {
        case "grayscale":
            canvas.getContext('2d').filter="grayscale(50)"
            //  canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
            canvas.getContext('2d').drawImage(img, 0, 0)
            break
        
        case "tfjs-pixel":
            tensor_draw_pixel(canvas, img)
            break
    }
}

/*
    TODO: Base function for TFJS based draws.
    Should handle BodyPix variations (Overlay/Pixels) and Pose-Animator.
*/
async function tensor_draw_pixel(canvas, img)
{
    try
    {
        const partSegmentation = await net.segmentMultiPersonParts(img);

        // The colored part image is an rgb image with a corresponding color from the
        // rainbow colors for each part at each pixel, and black pixels where there is
        // no part.
        const coloredPartImage = bodyPix.toColoredPartMask(partSegmentation);
        const opacity = 1;
        const flipHorizontal = false;
        const maskBlurAmount = 0;
        // Draw the colored part image on top of the original image onto a canvas.
        // The colored part image will be drawn semi-transparent, with an opacity of
        // 0.7, allowing for the original image to be visible under.
        bodyPix.drawPixelatedMask(
            canvas, img, coloredPartImage, opacity, maskBlurAmount,
            flipHorizontal);
    }
    catch(error)
    {   
        // Should be filling with a black/gray rect entirely, to keep User anonymous...
        console.log(error)
        canvas.getContext('2d').drawImage(img, 0, 0)
    }
}



override_getUserMedia()

async function loadPix(){
    //   net = await bodyPix.load();
    net = await bodyPix.load({
    architecture: 'MobileNetV1',
    outputStride: 16,
    multiplier: 1,
    quantBytes: 2,
    internalResolution: 'high'
    });
    console.log("Loaded BodyPix!")
}

/*
    An alternative timing loop, based on AudioContext's clock
    @arg callback : a callback function with the audioContext's currentTime passed as unique argument
    @arg frequency : float in ms;
*/
function audioTimerLoop(callback, frequency) {

    var freq = frequency/1000   //  AudioContext time parameters are in seconds
    var aCtx = new AudioContext()
    //  Chrome needs our oscillator node to be attached to the destination
    //  So we create a silent Gain Node
    var silence = aCtx.createGain()
    silence.gain.value = 0
    silence.connect(aCtx.destination)

    onOSCend();

    function onOSCend() 
    {
        var osc = aCtx.createOscillator()
        osc.onended = onOSCend  //  so we can loop
        osc.connect(silence)
        osc.start(0)    //  start it now
        console.log("In Timer loop")
        osc.stop(aCtx.currentTime + freq)   //  stop it next frame
        callback(aCtx.currentTime)  //  one frame is done

        if (!video_on) {    //  user broke the loop
            osc.onended = function() {
            aCtx.close();   //  clear the audioContext
            console.log("Exiting Timer loop", video_on)
            return
            };
        }
    };
    
}

function get_cam()
{   
    var audio_constraints = {"audio":{"mandatory":{"sourceId":"default"},"optional":[{"googEchoCancellation":true},{"googEchoCancellation2":true},{"googAutoGainControl":true},{"googNoiseSuppression":true},{"googHighpassFilter":true},{"googAudioMirroring":true}]},"video":false}
    var vid_constraints = {"audio":true,"video":{"advanced":[{"frameRate":{"min":24}},{"height":{"min":720}},{"width":{"min":1280}},{"frameRate":{"max":24}},{"width":{"max":1280}},{"height":{"max":720}},{"aspectRatio":{"exact":1.7777777777777777}}]}}
    var video_cont = document.querySelector("#videoElement");
    if (navigator.mediaDevices.getUserMedia) {
    navigator.mediaDevices.getUserMedia(vid_constraints)
        .then(function (stream) {
        video_cont.srcObject = stream;
        })
        .catch(function (err0r) {
        console.log("Something went wrong!", err0r.message)
        });
    }
}
</script>


<style>
#container {
    margin: 0px;
    width: 1280px;
    height: 720px;
    border: 10px #333 solid;
}
#videoElement {
    width: 1280px;
    height: 720px;
    background-color: #666;
}

</style>
</head>
 
<body id="body">
<div id="container">
	<video autoplay="true" id="videoElement">
	
	</video>
</div>

<div>
    <button onclick="get_cam()" >Click to stream video!</button>
</div>
</body>
<footer>test</footer>
</html>